%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% AMS Beamer series / Bologna FC / Template
% Andrea Omicini
% Alma Mater Studiorum - Universit√† di Bologna
% mailto:andrea.omicini@unibo.it
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{default}}
%
\documentclass[presentation]{beamer}\mode<presentation>{\usetheme{AMSBolognaFC}}
%\documentclass[handout]{beamer}\mode<handout>{\usetheme{AMSBolognaFC}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[T1]{fontenc}
\usepackage{wasysym}
\usepackage{amsmath,blkarray}
\usepackage{centernot}
\usepackage{fontawesome}
\usepackage{fancyvrb}
\usepackage[ddmmyyyy]{datetime}
\usepackage{comment}
\usepackage{listings}
\input{scala-def}
\lstset{language=scafi}

\renewcommand{\dateseparator}{}
%\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\version}{1}
\usepackage[
	backend=biber,
	%citestyle=authoryear-icomp,
	maxcitenames=1,
	bibstyle=alphabetic]{biblatex}

	\makeatletter

\addbibresource{bibliography.bib}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[]
{Proximal Policy Optimization}
%
\subtitle[]
{Algorithm implementation}
%
\author[\sspeaker{Domini}]
{\speaker{Davide Domini} \href{mailto:davide.domini@studio.unibo.it}{davide.domini@studio.unibo.it}}
%
\institute[DISI, Univ.\ Bologna]
{Department of Computer Science and Engineering - DISI\\\textsc{Alma Mater Studiorum} -- University of Bologna
\\[0.5cm]
\textbf{Deep Learning Course}}

%
\renewcommand{\dateseparator}{/}
\date[\today]{\today}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%/////////
\frame{\titlepage}
%/////////

%%===============================================================================
\section*{Outline}
%%===============================================================================

%%/////////
\frame[c]{\tableofcontents[hideallsubsections]}
%%/////////

%===============================================================================
\section{Background}
%===============================================================================

%/////////
\begin{frame}{Proximal Policy Optimization}
%/////////
	\begin{block}{Main features}
		\begin{itemize}
			\item \emph{Policy-based} method;
			\item \emph{On-policy} algorithm;
			\item \emph{Actor-Critic} algorithm;
			\item \emph{Clipped} surrogate objective function.
		\end{itemize}
	\end{block}
\end{frame}
%/////////

%/////////
\begin{frame}[allowframebreaks]{Actor-Critic methods} 
%/////////
\begin{columns}
	\begin{column}{0.55\textwidth}
		\begin{block}{Overview}
			\begin{itemize}
				\item Combination of \emph{value} and \emph{policy} based methods;
				\item \emph{Actor} controls how the agent behaves;
				\item \emph{Critic} evaluates the actions taken by the actor. 
			\end{itemize}
		\end{block}
	\end{column}
	\begin{column}{0.45\textwidth}
	\includegraphics[width=\textwidth]{img/ac_method.pdf}
	\end{column}
\end{columns}
	
\centering

\begin{alertblock}{Learning}
	 	\begin{itemize}
	 		\item The actor outputs the \emph{probability} of taking an \emph{action} in a given \emph{state};
	 		\item The critic estimates the \emph{advantage} of taking that action in that state
	 		\begin{itemize}
				\item $Q(s,a)$ represents the \emph{expected cumulative reward} an agent can expect in a given state;
				\item $V(s)$ represents the \emph{expected future reward} for a given state, \emph{regardless} of the action taken;
			\end{itemize}
	 		\item The advantage function is \emph{backpropagated} to both the networks.
	 	\end{itemize}
\end{alertblock}

\begin{exampleblock}{Advantage function}
	\begin{itemize}
		\item A positive value indicates that  taking that action in that state is expected to yield a better
			outcome than the average action;
		\item Otherwise the agent needs to explore 
			other actions or update the policy to improve the performance.
	\end{itemize}
\end{exampleblock}

\end{frame}
%/////////

%/////////
\begin{frame}{Pseudocode}
%/////////
\begin{figure}
	\includegraphics[width=0.9\textwidth]{img/PPO-pseudocode.pdf}
\end{figure}
\end{frame}
%/////////

%/////////
\begin{frame}{Reward to go}
%/////////

\begin{block}{Reward to go}
	The \emph{reward to go} estimates the discounted future reward that can be obtained 
		starting from a given state $S_k$.
\end{block}

\begin{figure}
	\includegraphics[width=0.6\textwidth]{img/reward-to-go.pdf}
\end{figure}	
\end{frame}
%/////////

%/////////
\begin{frame}{Clipped objective function}
%/////////

\begin{block}{Clipped objective function}
	\begin{itemize}
		\item The left part of the $min$ function multiplies the \emph{advantage} obtained by the \emph{ratio} between 
			the \emph{probability} of the action given by the \emph{current policy} and the probability of the action 
			given by the \emph{old policy}. 
		\item Using only this formula has a drawback; if the action is \emph{much more probable} in the new policy,
			then the weight update would be too large, making the training \emph{unstable}.
		\item Some algorithms use complex methods like the \emph{KL divergence} to avoid this problem; 
		\item \emph{PPO} uses a simpler approach: it \emph{clips} the probability ratio. 
	\end{itemize}
\end{block}

\begin{figure}
	\includegraphics[width=0.6\textwidth]{img/clippedobjfun.png}
\end{figure}	
\end{frame}
%/////////


%===============================================================================
\section{Project}
%===============================================================================

%/////////
\begin{frame}{Tools}
%/////////
\begin{block}{Tools}
	\begin{itemize}
		\item \textbf{PyTorch}
		\item \textbf{OpenAI Gym}
		\item \textbf{TensorBoard}
		\item \textbf{OpenCV}
		\item \textbf{Numpy}
	\end{itemize}
\end{block}
\end{frame}
%/////////

%/////////
\begin{frame}{Test Environment}
%/////////

\begin{block}{OpenAI Gym}
	\begin{itemize}
		\item A toolkit developed by \emph{OpenAI};
		\item Provides a wide variety of different environments for \emph{developing} and \emph{comparing} 
			reinforcement learning algorithms.
	\end{itemize}
\end{block}

\begin{columns}
	\begin{column}{0.55\textwidth}
		\begin{alertblock}{Bipedal Walker}
			\begin{itemize}
				\item This is a simple \emph{4-joint walker robot} environment;
				\item The walker starts standing at the left end of the terrain;
				\item The episode will terminate if the hull gets in contact with the 
					ground or if the walker exceeds the right end of the terrain length.
			\end{itemize}
		\end{alertblock}
	\end{column}
	\begin{column}{0.45\textwidth}
	\includegraphics[width=\textwidth]{img/bipedal.jpeg}
	\end{column}
\end{columns}

\end{frame}
%/////////

%/////////
\begin{frame}{Implementation}
%/////////

\begin{block}{ProximalPolicyOptimization class}
	\begin{itemize}
		\item Takes the \emph{environment} as input;
		\item Initializes the \emph{Actor} and the \emph{Critic} networks;
		\item The method \texttt{learn}:
		\begin{itemize}
			\item Takes as input the number of \emph{total timesteps};
			\item Runs the learning;
			\item Updates the networks weights;
		\end{itemize}
		\item The method \texttt{rollout} collects a set \emph{trajectories};
		\item The method \texttt{get\_action} returns the \emph{action} to take in 
			a given \emph{state};
		\item The method \texttt{evaluate\_action} computes the value function of the \emph{action} 
			in a given \emph{state}.
	\end{itemize}
\end{block}

% descrivi la classe PPO
% esempio di codice di come runnare l'esperimento

\end{frame}
%/////////

%/////////
\begin{frame}[fragile]{Normalization}
%/////////
\begin{block}{Advantages Normalization}
	\begin{itemize}
		\item Using raw advantges makes training higly \emph{unstable};
		\item Numerical algorithms behave poorly when \emph{different dimensions} 
			are also \emph{different in scale};
		\item \emph{Normalization} is not in the pseudocode but \emph{in practice} it is very important.
	\end{itemize}
\end{block}

\begin{lstlisting}
A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)
\end{lstlisting}

\end{frame}
%/////////


%===============================================================================
\section*{}
%===============================================================================

%/////////
\frame{\titlepage}
%/////////

%===============================================================================
\section*{\refname}
%===============================================================================

%%%%
\setbeamertemplate{page number in head/foot}{}
% %/////////
% \begin{frame}[c,noframenumbering, allowframebreaks]{\refname}
% %\begin{frame}[t,allowframebreaks,noframenumbering]{\refname}
% 	\tiny
% 	\nocite{*}
% 	\printbibliography
% \end{frame}
% %/////////

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
